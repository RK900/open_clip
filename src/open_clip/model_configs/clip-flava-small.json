{
  "embed_dim": 512,
  "vision_cfg": {
      "image_size": 224,
      "layers": 8,
      "width": 768,
      "head_width": 96,
      "patch_size": 16
  },
  "text_cfg": {
      "hf_model_name": "roberta-base",
      "hf_tokenizer_name": "roberta-base",
      "hf_model_pretrained": false,
      "hf_model_config": {
          "hidden_size": 768,
          "num_hidden_layers": 6,
          "num_attention_heads": 6,
          "intermediate_size": 3072,
          "max_position_embeddings": 128
      },
      "context_length": 77,
      "pooler_type": "cls_pooler",
      "proj": "mlp"
  }
}
